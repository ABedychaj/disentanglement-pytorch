{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from math import gamma\n",
    "\n",
    "def random_choice_full(input, n_samples, number_of_gausses):\n",
    "        from torch import multinomial, ones\n",
    "        if n_samples * number_of_gausses < input.shape[0]:\n",
    "            replacement = False\n",
    "        else:\n",
    "            replacement = True\n",
    "        idx = multinomial(ones(input.shape[0]), n_samples * number_of_gausses, replacement=replacement)\n",
    "        sampled = input[idx].reshape(number_of_gausses, n_samples, -1)\n",
    "        return torch.mean(sampled, axis=1)\n",
    "#         return sampled\n",
    "\n",
    "def provide_weights_for_x(x, how=None, device=None, n_samples=None, times=None):\n",
    "    dim = x.shape[1]\n",
    "    \n",
    "    if n_samples is None:\n",
    "        n_samples = dim\n",
    "    if times is None:\n",
    "        times = dim\n",
    "        \n",
    "    scale = (1 / dim)\n",
    "    sampled_points = random_choice_full(x, n_samples, times)\n",
    "\n",
    "    if how == \"gauss\":\n",
    "        from torch.distributions import MultivariateNormal\n",
    "        \n",
    "        cov_mat = (scale * torch.eye(dim)).repeat(dim, 1, 1)\n",
    "        mvn = MultivariateNormal(loc=sampled_points.to(device), covariance_matrix=cov_mat.to(device))\n",
    "        weight_vector = torch.exp(mvn.log_prob(x.reshape(-1, 1, dim).to(device)))\n",
    "        \n",
    "    elif how == \"sqrt\":\n",
    "        weight_vector = torch.sqrt(1 + sampled_points.reshape(-1, 1, dim).to(device) ** 2) ** (-1)\n",
    "        \n",
    "    elif how == \"log\":\n",
    "        weight_vector = torch.log(1 + sampled_points.reshape(-1, 1, dim).to(device)**2)\n",
    "        \n",
    "    elif how == \"TStudent\":\n",
    "        from torch.distributions.studentT import StudentT\n",
    "        \n",
    "        mvn = StudentT(df=1, loc=x.mean(0), scale=scale)\n",
    "        weight_vector = torch.exp(mvn.log_prob(x.reshape(-1, 1, dim).to(device)))\n",
    "        # to trzeba poprawić ?!\n",
    "    elif how == \"Cauchy\":\n",
    "        from torch.distributions.cauchy import Cauchy\n",
    "        \n",
    "        mvn = Cauchy(loc=x.mean(0), scale=1)\n",
    "        weight_vector = torch.exp(mvn.log_prob(x.reshape(-1, 1, dim).to(device)))\n",
    "    elif how == \"Gumbel\":\n",
    "        from torch.distributions.gumbel import Gumbel\n",
    "        \n",
    "        mvn = Gumbel(loc=x.mean(0), scale=1)\n",
    "        weight_vector = torch.exp(mvn.log_prob(x.reshape(-1, 1, dim).to(device)))\n",
    "    elif how == \"Laplace\":\n",
    "        from torch.distributions.laplace import Laplace\n",
    "        \n",
    "        mvn = Laplace(loc=x.mean(0), scale=1)\n",
    "        weight_vector = torch.exp(mvn.log_prob(x.reshape(-1, 1, dim).to(device)))\n",
    "    return sampled_points, weight_vector\n",
    "    \n",
    "class WICA(object):\n",
    "    def __init__(self):\n",
    "        self.number_of_gausses = 10\n",
    "        self.z_dim = 5\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def wica_loss(self, z, latent_normalization=False, how=\"gauss\"):\n",
    "        if latent_normalization:\n",
    "            x = (z - z.mean(dim=1, keepdim=True)) / z.std(dim=1, keepdim=True)\n",
    "        else:\n",
    "            x = z\n",
    "        dim = self.z_dim if self.z_dim is not None else x.shape[1]\n",
    "\n",
    "\n",
    "        _, weight_vector = provide_weights_for_x(\n",
    "                x=x, \n",
    "                how=how,\n",
    "                device = self.device\n",
    "        )\n",
    "                        \n",
    "        sum_of_weights = torch.sum(weight_vector, axis=0)\n",
    "        \n",
    "        weight_sum = torch.sum(x.reshape(1,x.shape[0], x.shape[1]) * weight_vector, axis=0)\n",
    "\n",
    "        weight_mean = weight_sum / sum_of_weights\n",
    "\n",
    "        xm = x - weight_mean\n",
    "        wxm = torch.sum(xm.reshape(1,xm.shape[0], xm.shape[1]) * weight_vector, axis=0)\n",
    "\n",
    "        wcov = (wxm.reshape(1 ,wxm.shape[0], wxm.shape[1]).permute(0, 2, 1).matmul(xm)) / sum_of_weights\n",
    "\n",
    "        diag = torch.diagonal(wcov ** 2, dim1=1, dim2=2)\n",
    "        diag_pow_plus = diag.reshape(diag.shape[0], diag.shape[1], -1) + diag.reshape(diag.shape[0], -1, diag.shape[1])\n",
    "\n",
    "        tmp = (2 * wcov ** 2 / diag_pow_plus)\n",
    "\n",
    "        triu = torch.triu(tmp, diagonal=1)\n",
    "        normalize = 2.0 / (dim * (dim - 1))\n",
    "        cost = torch.sum(normalize * triu) / self.number_of_gausses\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0011)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, weight_vector = provide_weights_for_x(t, 'sqrt')\n",
    "sum_of_weights = torch.sum(weight_vector, axis=0)\n",
    "\n",
    "weight_sum = torch.sum(t.reshape(1,t.shape[0], t.shape[1])*weight_vector, axis=0)\n",
    "weight_mean = weight_sum / sum_of_weights\n",
    "xm = t-weight_mean\n",
    "wxm = torch.sum(xm.reshape(1,xm.shape[0], xm.shape[1])*weight_vector, axis=0)\n",
    "\n",
    "wcov = (wxm.reshape(1 ,wxm.shape[0], wxm.shape[1]).permute(0, 2, 1).matmul(xm)) / sum_of_weights\n",
    "diag = torch.diagonal(wcov ** 2, dim1=1, dim2=2)\n",
    "\n",
    "diag_pow_plus = diag.reshape(diag.shape[0], diag.shape[1], -1) + diag.reshape(diag.shape[0], -1, diag.shape[1])\n",
    "tmp = (2 * wcov ** 2 / diag_pow_plus)\n",
    "\n",
    "triu = torch.triu(tmp, diagonal=1)\n",
    "normalize = 2.0 / (3 * (3 - 1))\n",
    "cost = torch.sum(normalize * triu) / 5\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1773, -0.6457, -0.8364, -0.9291],\n",
       "        [ 0.2543, -0.0167, -0.9255,  0.7730],\n",
       "        [ 0.9994,  0.2729,  0.0856,  0.6789],\n",
       "        ...,\n",
       "        [-0.5104, -0.9665,  0.0550,  0.8703],\n",
       "        [-0.1197, -0.4765,  0.8096,  0.9986],\n",
       "        [-0.9210, -0.2407,  0.9671, -0.8304]])"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wica = WICA()\n",
    "t = torch.sin(torch.randn(1200)).resize(300,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled_points: \n",
      " tensor([[[-0.1189, -0.9492,  0.8771]],\n",
      "\n",
      "        [[ 0.9893, -0.7897,  0.0857]],\n",
      "\n",
      "        [[-0.2262, -0.6038, -0.9691]],\n",
      "\n",
      "        [[-0.3576, -0.4485, -0.9810]],\n",
      "\n",
      "        [[ 0.8463,  0.6288, -0.0036]]])\n",
      "weight_vector: \n",
      " tensor([[[1.4048e-02, 6.4240e-01, 5.7057e-01]],\n",
      "\n",
      "        [[6.8247e-01, 4.8469e-01, 7.3247e-03]],\n",
      "\n",
      "        [[4.9899e-02, 3.1083e-01, 6.6221e-01]],\n",
      "\n",
      "        [[1.2031e-01, 1.8329e-01, 6.7419e-01]],\n",
      "\n",
      "        [[5.4013e-01, 3.3318e-01, 1.3113e-05]]])\n",
      "t: \n",
      " tensor([[-0.6933,  0.3938,  0.8895],\n",
      "        [-0.5661, -0.7051,  0.8036],\n",
      "        [-0.2262, -0.6038, -0.9691],\n",
      "        [ 0.3380,  0.9316, -0.2815],\n",
      "        [ 0.4570,  0.1752,  0.9920],\n",
      "        [-0.9608, -0.6589,  0.4055],\n",
      "        [ 0.9893, -0.7897,  0.0857],\n",
      "        [ 0.8710,  0.9984, -0.5808],\n",
      "        [ 0.9694,  0.9705, -0.9842],\n",
      "        [ 0.9922, -0.3587,  0.9572],\n",
      "        [ 0.0845,  0.7502,  0.9995],\n",
      "        [-0.9964,  0.6247, -0.3304],\n",
      "        [ 0.7349, -0.7074,  0.8904],\n",
      "        [-0.9408, -0.8618,  0.7585],\n",
      "        [-0.5967,  0.8138,  0.4863],\n",
      "        [-0.3455,  0.0066,  0.6138],\n",
      "        [-0.0651,  0.6400, -0.9519],\n",
      "        [ 0.1633, -0.0632,  0.6262],\n",
      "        [-0.9289, -0.8787, -0.2743],\n",
      "        [-0.3576, -0.4485, -0.9810],\n",
      "        [ 0.7682,  0.2408,  0.5921],\n",
      "        [-0.3158,  0.5621, -0.6283],\n",
      "        [-0.1189, -0.9492,  0.8771],\n",
      "        [ 0.4592, -0.8521, -0.1370],\n",
      "        [ 0.9997,  0.7996, -0.5122],\n",
      "        [ 0.5176,  0.0803,  0.3732],\n",
      "        [-0.9639, -0.9717,  0.8040],\n",
      "        [-0.3259, -0.1704, -0.1438],\n",
      "        [ 0.2156,  0.3555, -0.7504],\n",
      "        [ 0.8463,  0.6288, -0.0036]])\n"
     ]
    }
   ],
   "source": [
    "dim = t.shape[1]\n",
    "\n",
    "sampled_points = random_choice_full(t, 1, 5)\n",
    "weight_vector = torch.log(1 + sampled_points.reshape(-1, 1, dim)**2)\n",
    "print(\"sampled_points: \\n\", sampled_points)\n",
    "print(\"weight_vector: \\n\", weight_vector)\n",
    "print(\"t: \\n\", t)\n",
    "# własciwa funkcja\n",
    "# weight_sum = t.T.matmul(weight_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0920)\n",
      "tensor(0.1662)\n",
      "tensor(0.4097)\n",
      "tensor(0.3237)\n",
      "tensor(0.0496)\n",
      "tensor(0.1337)\n",
      "tensor(0.2578)\n",
      "tensor(0.7654)\n",
      "tensor(0.2746)\n",
      "tensor(1.8804)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(1000*wica.wica_loss(t, latent_normalization=True, how=\"sqrt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 10, 2]' is invalid for input of size 1200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-406-3855917ea8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# t.reshape(1,5,2).matmul(weight_vector)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 10, 2]' is invalid for input of size 1200"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1,10,2)[0])\n",
    "print(weight_vector)\n",
    "\n",
    "t.reshape(1,10,2) * weight_vector\n",
    "# t.reshape(1,5,2).matmul(weight_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'sampled_points' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-315-0de896d29b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwica_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_normalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sqrt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-188-1c5e1a3855de>\u001b[0m in \u001b[0;36mwica_loss\u001b[0;34m(self, z, latent_normalization, how)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-1c5e1a3855de>\u001b[0m in \u001b[0;36mprovide_weights_for_x\u001b[0;34m(x, how, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmvn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLaplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mweight_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmvn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msampled_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWICA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'sampled_points' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(wica.wica_loss(t, latent_normalization=True, how=\"sqrt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(0, '/Users/andrzej/Personal/Projects/disentanglement-pytorch')\n",
    "from models.ae import AEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AEModel(\n",
       "  (encoder): SimpleConv64(\n",
       "    (main): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Flatten3D()\n",
       "      (13): Linear(in_features=256, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): SimpleConv64(\n",
       "    (main): Sequential(\n",
       "      (0): Unsqueeze3D()\n",
       "      (1): Conv2d(8, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (10): ReLU(inplace=True)\n",
       "      (11): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from architectures import encoders, decoders\n",
    "\n",
    "encoder_name = \"SimpleConv64\"\n",
    "decoder_name = \"SimpleConv64\"\n",
    "\n",
    "encoder = getattr(encoders, encoder_name)\n",
    "decoder = getattr(decoders, decoder_name)\n",
    "\n",
    "model = AEModel(encoder(8, 1, 64), decoder(8, 1, 64)).to(torch.device('cpu'))\n",
    "\n",
    "checkpoint = torch.load('/Users/andrzej/Personal/results/sqrt-first-batch-wica/last', map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_states']['G'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.data_loader import get_dataloader\n",
    "\n",
    "train_loader = get_dataloader('dsprites_full', '/Users/andrzej/Personal/Projects/data/test_dsets', 3,\n",
    "                              123, num_workers=1, pin_memory=True, image_size=64, \n",
    "                              include_labels=None, shuffle=True, droplast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "a,b = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils\n",
    "def visualize_recon(input_image, recon_image):\n",
    "        input_image = torchvision.utils.make_grid(input_image)\n",
    "        recon_image = torchvision.utils.make_grid(recon_image)\n",
    "\n",
    "        white_line = torch.ones((3, input_image.size(1), 10)).to('cpu')\n",
    "        samples = torch.cat([input_image, white_line, recon_image], dim=2)\n",
    "\n",
    "        torchvision.utils.save_image(samples, \"test.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_recon(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.utils import grid2gif, get_data_for_visualization, prepare_data_for_visualization\n",
    "import os\n",
    "\n",
    "z_dim = 8\n",
    "l_dim = 0\n",
    "traverse_z = True\n",
    "traverse_c = False\n",
    "num_labels = 0\n",
    "image_size = 64\n",
    "num_channels = train_loader.dataset.num_channels()\n",
    "\n",
    "def set_z(z, latent_id, val):\n",
    "    z[:, latent_id] = val\n",
    "\n",
    "def encode_deterministic(**kwargs):\n",
    "    images = kwargs['images']\n",
    "    if len(images.size()) == 3:\n",
    "        images = images.unsqueeze(0)\n",
    "    return model.encode(images)\n",
    "\n",
    "def decode_deterministic(**kwargs):\n",
    "    latent = kwargs['latent']\n",
    "    if len(latent.size()) == 1:\n",
    "        latent = latent.unsqueeze(0)\n",
    "    return model.decode(latent)\n",
    "\n",
    "def visualize_traverse(limit: tuple, spacing, data=None, test=False):\n",
    "    interp_values = torch.arange(limit[0], limit[1]+spacing, spacing)\n",
    "    num_cols = interp_values.size(0)\n",
    "\n",
    "    sample_images_dict, sample_labels_dict = prepare_data_for_visualization(next(iter(train_loader)))\n",
    "    encodings = dict()\n",
    "\n",
    "    for key in sample_images_dict.keys():\n",
    "        encodings[key]  = encode_deterministic(images=sample_images_dict[key], labels=sample_labels_dict[key])\n",
    "\n",
    "    gifs = []\n",
    "    for key in encodings:\n",
    "        latent_orig = encodings[key]\n",
    "        label_orig = sample_labels_dict[key]\n",
    "        print('latent_orig: {}, label_orig: {}'.format(latent_orig, label_orig))\n",
    "        samples = []\n",
    "\n",
    "        # encode original on the first row\n",
    "        sample = decode_deterministic(latent=latent_orig, labels=label_orig)\n",
    "        for _ in interp_values:\n",
    "            samples.append(sample)\n",
    "        for zid in range(z_dim):\n",
    "            for val in interp_values:\n",
    "                latent = latent_orig\n",
    "                latent[:, zid] = val\n",
    "                set_z(latent, zid, val)\n",
    "                sample = decode_deterministic(latent=latent, labels=label_orig)\n",
    "\n",
    "                samples.append(sample)\n",
    "                gifs.append(sample)\n",
    "                    \n",
    "        samples = torch.cat(samples, dim=0).cpu()\n",
    "        samples = torchvision.utils.make_grid(samples, nrow=num_cols)\n",
    "        \n",
    "        file_name = os.path.join(\".\", '{}_{}.{}'.format(\"traverse\", key, \"png\"))\n",
    "        torchvision.utils.save_image(samples, file_name)\n",
    "        \n",
    "    total_rows = num_labels * l_dim + \\\n",
    "                 z_dim * int(traverse_z) + \\\n",
    "                 num_labels * int(traverse_c)\n",
    "    gifs = torch.cat(gifs)\n",
    "    gifs = gifs.view(len(encodings), total_rows, num_cols,\n",
    "                     num_channels, image_size, image_size).transpose(1, 2)\n",
    "    for i, key in enumerate(encodings.keys()):\n",
    "        for j, val in enumerate(interp_values):\n",
    "            file_name = \\\n",
    "                os.path.join('.', '{}_{}_{}.{}'.format('tmp', key, str(j).zfill(2), '.png'))\n",
    "            torchvision.utils.save_image(tensor=gifs[i][j].cpu(),\n",
    "                                         filename=file_name,\n",
    "                                         nrow=total_rows, pad_value=1)\n",
    "            \n",
    "        file_name = os.path.join('.', '{}_{}.{}'.format('traverse', key, 'gif'))\n",
    "\n",
    "        grid2gif(str(os.path.join('.', '{}_{}*.{}').format('tmp', key, 'png')),\n",
    "                 file_name, delay=10)\n",
    "\n",
    "        # Delete temp image files\n",
    "        for j, val in enumerate(interp_values):\n",
    "            os.remove(\n",
    "                os.path.join('.', '{}_{}_{}.{}'.format('tmp', key, str(j).zfill(2), '.png')))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_orig: tensor([[-18.7481,   2.6798,  16.7035,  40.3148,  39.0097,  20.5979,  57.7464,\n",
      "         -64.9600]], grad_fn=<AddmmBackward>), label_orig: tensor([0])\n",
      "latent_orig: tensor([[-59.9391,  72.4069,  -4.9910,   9.8507,  15.5894, -51.6062, -55.8661,\n",
      "           8.1131]], grad_fn=<AddmmBackward>), label_orig: tensor([0])\n",
      "latent_orig: tensor([[ 47.1662,  -7.9580,  11.1962, -23.2260,  53.1356,  52.4977,  54.3462,\n",
      "         -66.5915]], grad_fn=<AddmmBackward>), label_orig: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "min_ = -50\n",
    "max_ = 50\n",
    "spacing_ = 5\n",
    "samples = visualize_traverse(limit=(min_,max_), spacing=spacing_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
